{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter-10 k-nearest neighbors(K最近邻算法)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 橙子还是柚子\n",
    "\n",
    "- 在区分橙子还是柚子时，我们通常知道，柚子比橙子更大、更红。  \n",
    "  ![](2022-02-20-10-57-19.png)\n",
    "\n",
    "- 如果出现一个神秘的水果，我们在判断水果是橙子和柚子时，通常是看它的邻居，观察离它最近的三个邻居。  \n",
    "  ![](2022-02-20-10-59-14.png)\n",
    "\n",
    "- KNN算法的步骤：  \n",
    "  ![](2022-02-20-11-02-27.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 创建推荐系统\n",
    "\n",
    "- 问题描述：加入你是Netfllix，要为用户创建一个电影推荐系统。从本质上说，类似于前面的水果问题。\n",
    "\n",
    "1. 将所有用户放入一个图表中，喜好相似的用户距离较近。  \n",
    "   ![](2022-02-20-11-06-55.png)\n",
    "\n",
    "### 2.1 特征抽取\n",
    "- ![](2022-02-20-11-11-52.png)\n",
    "- ![](2022-02-20-11-12-13.png)\n",
    "- ![](2022-02-20-11-12-37.png)\n",
    "\n",
    "- 在多维空间中，距离指出了两组数字之间的相似程度。\n",
    "\n",
    "- 给电影打分时，每位用户的标准都不相同。假设有两位用户——Yogi和Pinky，他们欣赏电影的品味相同，但Yogi给喜欢的电影都打5分，而Pinky更挑剔，只给特别好的电影打5分。他们的品味一致，但根据距离算法，他们并非邻居。这时，我们可以使用**归一化(normalization)**,计算每位用户的**平均评分**，并根据此来调整用户的评分。例如，可能发现Pinky的平均评分为3星，而Yogi的平均评分为3.5星。因此，我们稍微调高Pinky的评分，使其平均评分也为3.5星。这样就能基于同样的标准比较他们的评分了。\n",
    "\n",
    "### 2.2 回归\n",
    "- 假设不仅要向Priyanka推荐电影，还要预测她将给这部电影打多少分。\n",
    "\n",
    "- 求出别人打分的平均值，就是简单的**回归(regression)**  \n",
    "  ![](2022-02-20-11-37-46.png)\n",
    "\n",
    "- 我们使用KNN来做两项基本工作——分类和回归：\n",
    "  - [ ] 分类就是编组；\n",
    "  - [ ] 回归就是预测结果(如一个数字)\n",
    "\n",
    "- 回归问题描述：  \n",
    "  假设我们在伯克利开个面包店，每天都需要新鲜的面包，需要根据如下一组特征预测当天该烤多少条面包：\n",
    "  - [ ] 天气指数1-5(5表示天气很好，1表示天气很糟)\n",
    "  - [ ] 是不是周末或节假日(周末或节假日为1，否则为0)\n",
    "  - [ ] 有没有活动(1表示有，0表示没有)  \n",
    "  - 一些历史数据：\n",
    "      ![](2022-02-20-11-53-10.png)\n",
    "\n",
    "  - ![](2022-02-20-11-59-02.png)\n",
    "\n",
    "  - 解体步骤：用KNN算法，其中K为4。首先找出与今天最接近的4个邻居。\n",
    "    - ![](2022-02-20-12-00-14.png)\n",
    "    - 计算这些天售出的面包的平均数，结果218.75，即为我们今天要烤的面包树！\n",
    "\n",
    "### 2.3 余弦相似度(cosine similarity)\n",
    "\n",
    "- 在实际工作中，我们经常使用**余弦相似度**(cosine similarity)。\n",
    "- 余弦相似度不计算两个矢量的距离，而是比较它们的角度，因此更适合处理前面所说的情况。\n",
    "\n",
    "### 2.4 挑选合适的特征\n",
    "\n",
    "- 使用KNN时，挑选合适的特征进行比较至关重要。所谓合适的特征就是：\n",
    "  - [ ] 与要推荐的电影紧密相关的特征；\n",
    "  - [ ] 不偏不倚的特征(例如，如果只让用户给喜剧片打分，就无法判断他们是否喜欢动作片)\n",
    "\n",
    "- 在挑选合适的特征方面，没有放之四海皆准的法则，必须考虑到各种需要考虑的因素。\n",
    "\n",
    "- KNN算法一个不错的经验规则是：如果有N位用户，应考虑Sqrt(N)个邻居。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 机器学习简介\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 小结\n",
    "- [ ] KNN用于分类和回归，需要考虑最近的邻居。\n",
    "- [ ] 分类就是编组。\n",
    "- [ ] 回归就是预测结果(如数字)\n",
    "- [ ] 特征抽取意味着将物品(如水果或用户)转换为一系列可比较的数字\n",
    "- [ ] 能否挑选合适的特征事关KNN算法的成败"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
